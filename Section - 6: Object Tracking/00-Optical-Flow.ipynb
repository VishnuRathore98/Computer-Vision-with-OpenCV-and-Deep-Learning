{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical Flow\n",
    "\n",
    "----\n",
    "#### NOTE: It is probably a good idea to restart the kernel if you ever run these cells, as the tracking algo can sometimes get caught in a loop with your camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lucas-Kanade Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for ShiTomasi corner detection (good features to track paper)\n",
    "corner_track_params = dict(maxCorners = 10,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for Lucas Kanade Optical Flow\n",
    "\n",
    "Detect the motion of specific points or the aggregated motion of regions by modifying the winSize argument. This determines the integration window size. Small windows are more sensitive to noise and may miss larger motions. Large windows will “survive” an occlusion.\n",
    "\n",
    "The integration appears smoother with the larger window size.\n",
    "\n",
    "criteria has two here - the max number (10 above) of iterations and epsilon (0.03 above). More iterations means a more exhaustive search, and a smaller epsilon finishes earlier. These are primarily useful in exchanging speed vs accuracy, but mainly stay the same.\n",
    "\n",
    "When maxLevel is 0, it is the same algorithm without using pyramids (ie, calcOpticalFlowLK). Pyramids allow finding optical flow at various resolutions of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (200,200),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10,0.03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Grab the very first frame of the stream\n",
    "ret, prev_frame = cap.read()\n",
    "\n",
    "# Grab a grayscale image (We will refer to this as the previous frame)\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Grabbing the corners\n",
    "prevPts = cv2.goodFeaturesToTrack(prev_gray, mask = None, **corner_track_params)\n",
    "\n",
    "# Create a matching mask of the previous frame for drawing on later\n",
    "mask = np.zeros_like(prev_frame)\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Grab current frame\n",
    "    ret,frame = cap.read()\n",
    "    \n",
    "    # Grab gray scale\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate the Optical Flow on the Gray Scale Frame\n",
    "    nextPts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, frame_gray, prevPts, None, **lk_params)\n",
    "    \n",
    "    # Using the returned status array (the status output)\n",
    "    # status output status vector (of unsigned chars); each element of the vector is set to 1 if\n",
    "    # the flow for the corresponding features has been found, otherwise, it is set to 0.\n",
    "    good_new = nextPts[status==1]\n",
    "    good_prev = prevPts[status==1]\n",
    "    \n",
    "    # Use ravel to get points to draw lines and circles\n",
    "    for i,(new,prev) in enumerate(zip(good_new,good_prev)):\n",
    "        \n",
    "        x_new,y_new = new.ravel()\n",
    "        x_prev,y_prev = prev.ravel()\n",
    "        \n",
    "        # Lines will be drawn using the mask created from the first frame\n",
    "        mask = cv2.line(mask, (x_new,y_new),(x_prev,y_prev), (0,255,0), 3)\n",
    "        \n",
    "        # Draw red circles at corner points\n",
    "        frame = cv2.circle(frame,(x_new,y_new),8,(0,0,255),-1)\n",
    "    \n",
    "    # Display the image along with the mask we drew the line on.\n",
    "    img = cv2.add(frame,mask)\n",
    "    cv2.imshow('frame',img)\n",
    "    \n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "   \n",
    "    # Now update the previous frame and previous points\n",
    "    prev_gray = frame_gray.copy()\n",
    "    prevPts = good_new.reshape(-1,1,2)\n",
    "    \n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Explanation of Optical Flow Code\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Shi-Tomasi Corner Detection**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "prevPts = cv2.goodFeaturesToTrack(prev_gray, mask=None, **corner_track_params)\n",
    "```\n",
    "\n",
    "- **Concept**:  \n",
    "  Shi-Tomasi is a corner detection algorithm that finds \"good features to track.\" Corners are points in the image where the intensity changes significantly in both directions (horizontal and vertical), making them easy to identify across frames.\n",
    "\n",
    "- **Why Corners?**  \n",
    "  - Corners are more stable and easier to track compared to flat regions or edges:\n",
    "    - A flat region (e.g., plain wall) looks similar in all directions, making it hard to track.\n",
    "    - An edge only has a change in one direction, so it’s less distinctive.\n",
    "    - A corner has a unique intensity change in all directions.\n",
    "\n",
    "- **Parameters Explained**:\n",
    "  - `maxCorners`: Limits the number of corners detected (e.g., 10 corners).\n",
    "  - `qualityLevel`: A threshold to discard weak corners. It’s a fraction of the strongest corner's quality. Higher values result in fewer, but stronger corners.\n",
    "  - `minDistance`: Ensures corners are not too close together.\n",
    "  - `blockSize`: Size of the window used to calculate the intensity gradient.\n",
    "\n",
    "- **How it Works**:\n",
    "  1. Computes the gradient (intensity change) in the image.\n",
    "  2. Looks for pixels where the gradient is strong in both directions.\n",
    "  3. Applies the `qualityLevel` and `minDistance` filters.\n",
    "\n",
    "- **Example Output**:  \n",
    "  An array of corner points, each with \\((x, y)\\) coordinates. These points will be used for tracking in the next frame.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Lucas-Kanade Optical Flow**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "nextPts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, frame_gray, prevPts, None, **lk_params)\n",
    "```\n",
    "\n",
    "- **Concept**:  \n",
    "  The Lucas-Kanade (LK) method tracks points across frames. It assumes that:\n",
    "  1. The motion between two frames is small.\n",
    "  2. Nearby points move in a similar way.\n",
    "\n",
    "  The algorithm finds the new position of each corner point in the next frame.\n",
    "\n",
    "- **How It Works**:\n",
    "  1. A small window (defined by `winSize`) is placed around each point to track.\n",
    "  2. The algorithm looks for the most similar region in the next frame by comparing brightness patterns inside the window.\n",
    "  3. It iteratively adjusts the position until the difference is minimized (controlled by `criteria`).\n",
    "\n",
    "- **Parameters Explained**:\n",
    "  - `prev_gray`: Previous frame (grayscale).\n",
    "  - `frame_gray`: Current frame (grayscale).\n",
    "  - `prevPts`: Points to track in the previous frame.\n",
    "  - `winSize`: Size of the search window for matching points.\n",
    "  - `maxLevel`: Number of image pyramid levels for multi-scale tracking. Higher levels allow tracking across varying sizes (e.g., zoomed-in/out objects).\n",
    "  - `criteria`: Stopping conditions for iterations:\n",
    "    - `TERM_CRITERIA_EPS`: Stops when the positional change is small (e.g., < 0.03).\n",
    "    - `TERM_CRITERIA_COUNT`: Stops after a fixed number of iterations (e.g., 10).\n",
    "\n",
    "- **Outputs**:\n",
    "  - `nextPts`: Estimated new positions of the points in the current frame.\n",
    "  - `status`: A binary array where `1` means the point was successfully tracked, and `0` means it wasn’t.\n",
    "  - `err`: Error values indicating how well the point was tracked.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Tracking Points Across Frames**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "good_new = nextPts[status == 1]\n",
    "good_prev = prevPts[status == 1]\n",
    "```\n",
    "\n",
    "- **Concept**:  \n",
    "  Not all points are successfully tracked between frames.  \n",
    "  - `status == 1` filters only the points that were successfully tracked.  \n",
    "    - `good_prev`: The positions of the tracked points in the previous frame.  \n",
    "    - `good_new`: The positions of the same points in the current frame.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Drawing the Motion**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "for i, (new, prev) in enumerate(zip(good_new, good_prev)):\n",
    "    x_new, y_new = new.ravel()\n",
    "    x_prev, y_prev = prev.ravel()\n",
    "    mask = cv2.line(mask, (x_new, y_new), (x_prev, y_prev), (0, 255, 0), 3)\n",
    "    frame = cv2.circle(frame, (x_new, y_new), 8, (0, 0, 255), -1)\n",
    "```\n",
    "\n",
    "- **What It Does**:\n",
    "  - Loops through each pair of points (previous and current positions).\n",
    "  - Draws a **line** on the `mask` from the previous position to the current position to represent motion.\n",
    "  - Draws a **red circle** on the `frame` at the current position.\n",
    "\n",
    "- **Key Details**:\n",
    "  - `zip(good_new, good_prev)`: Combines the current and previous points so they can be processed together.\n",
    "  - `ravel()`: Flattens the \\((x, y)\\) coordinates into simple numbers for easier drawing.\n",
    "  - `cv2.line`: Draws the green motion line on the mask.\n",
    "    - `(0, 255, 0)`: Green color.\n",
    "    - `3`: Thickness of the line.\n",
    "  - `cv2.circle`: Draws a red dot at the new position of the point.\n",
    "    - `(0, 0, 255)`: Red color.\n",
    "    - `-1`: Fills the circle.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Combining Mask and Frame**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "img = cv2.add(frame, mask)\n",
    "cv2.imshow('frame', img)\n",
    "```\n",
    "\n",
    "- **Concept**:  \n",
    "  Combines the motion lines (on `mask`) with the current frame (`frame`) and displays the result:\n",
    "  - `cv2.add`: Adds the two images together.\n",
    "  - `cv2.imshow`: Displays the combined image in a window called `'frame'`.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Loop Control and Update**\n",
    "\n",
    "### Key Updates:\n",
    "```python\n",
    "k = cv2.waitKey(30) & 0xff\n",
    "if k == 27:\n",
    "    break\n",
    "```\n",
    "\n",
    "- **`cv2.waitKey`**: Waits for 30 ms to check if a key is pressed.  \n",
    "  - `27`: ASCII code for the `Esc` key. If pressed, the loop exits.\n",
    "\n",
    "### Updating Variables:\n",
    "```python\n",
    "prev_gray = frame_gray.copy()\n",
    "prevPts = good_new.reshape(-1, 1, 2)\n",
    "```\n",
    "\n",
    "- **Why Update?**  \n",
    "  To track motion, the current frame becomes the new \"previous frame\" for the next iteration.\n",
    "  - `prev_gray`: The current grayscale frame is stored as the previous frame.\n",
    "  - `prevPts`: The new points (`good_new`) are reshaped into the format required for tracking in the next loop.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Cleanup**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "```\n",
    "\n",
    "- **Purpose**:\n",
    "  - `cv2.destroyAllWindows()`: Closes all OpenCV windows.\n",
    "  - `cap.release()`: Releases the video capture resource, freeing up the camera for other applications.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary of Workflow**\n",
    "1. Detect key points (corners) in the first frame.\n",
    "2. Continuously read new frames and convert them to grayscale.\n",
    "3. Use the Lucas-Kanade method to track the motion of these key points.\n",
    "4. Draw lines and circles to visualize the motion.\n",
    "5. Update frames and points to repeat the process.\n",
    "6. Exit and clean up resources when the user presses the `Esc` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Optical Flow in OpenCV\n",
    "\n",
    "calcOpticalFlowFarneback(prev, next, flow, pyr_scale, levels, winsize, iterations, poly_n, poly_sigma, flags) -> flow\n",
    "\n",
    "This function computes a dense optical flow using the Gunnar Farneback's algorithm.\n",
    "\n",
    "Here are the parameters for the function and what they represent:\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* prev first 8-bit single-channel input image.\n",
    "* next second input image of the same size and the same type as prev.\n",
    "* flow computed flow image that has the same size as prev and type CV_32FC2.\n",
    "* pyr_scale parameter, specifying the image scale (\\<1) to build pyramids for each image\n",
    "    * pyr_scale=0.5 means a classical pyramid, where each next layer is twice smaller than the previous one.\n",
    "    \n",
    "* levels number of pyramid layers including the initial image; levels=1 means that no extra layers are created and only the original images are used.\n",
    "* winsize averaging window size\n",
    "    * larger values increase the algorithm robustness to image\n",
    "* noise and give more chances for fast motion detection, but yield more blurred motion field.\n",
    "* iterations number of iterations the algorithm does at each pyramid level.\n",
    "* poly_n size of the pixel neighborhood used to find polynomial expansion in each pixel\n",
    "    * larger values mean that the image will be approximated with smoother surfaces, yielding more robust algorithm and more blurred motion field, typically poly_n =5 or 7.\n",
    "* poly_sigma standard deviation of the Gaussian that is used to smooth derivatives used as a basis for the polynomial expansion; for poly_n=5, you can set poly_sigma=1.1, for poly_n=7, a good value would be poly_sigma=1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "\n",
    "# Capture the frame\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame1 = cap.read()\n",
    "\n",
    "# Get gray scale image of first frame and make a mask in HSV color\n",
    "prvsImg = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "hsv_mask = np.zeros_like(frame1)\n",
    "hsv_mask[:,:,1] = 255\n",
    "\n",
    "while True:\n",
    "    ret, frame2 = cap.read()\n",
    "    nextImg = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Check out the markdown text above for a break down of these paramters, most of these are just suggested defaults\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvsImg,nextImg, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    \n",
    "    # Color the channels based on the angle of travel\n",
    "    # Pay close attention to your video, the path of the direction of flow will determine color!\n",
    "    mag, ang = cv2.cartToPolar(flow[:,:,0], flow[:,:,1],angleInDegrees=True)\n",
    "    hsv_mask[:,:,0] = ang/2\n",
    "    hsv_mask[:,:,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Convert back to BGR to show with imshow from cv\n",
    "    bgr = cv2.cvtColor(hsv_mask,cv2.COLOR_HSV2BGR)\n",
    "    cv2.imshow('frame2',bgr)\n",
    "    \n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    \n",
    "    # Set the Previous image as the next iamge for the loop\n",
    "    prvsImg = nextImg\n",
    "\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Optical Flow Using Farneback Method\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Importing Required Libraries**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "import cv2 \n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "- **`cv2`**: OpenCV library for image and video processing.\n",
    "- **`numpy`**: For numerical operations and creating masks.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Setting Up Video Capture**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame1 = cap.read()\n",
    "```\n",
    "\n",
    "- **`cv2.VideoCapture(0)`**: Captures video from the default camera (camera index `0`).\n",
    "- **`cap.read()`**:\n",
    "  - `ret`: Boolean indicating if the frame was successfully captured.\n",
    "  - `frame1`: First frame of the video stream.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Preparing the First Frame**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "prvsImg = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "hsv_mask = np.zeros_like(frame1)\n",
    "hsv_mask[:,:,1] = 255\n",
    "```\n",
    "\n",
    "- **Convert to Grayscale**:\n",
    "  - `cv2.cvtColor`: Converts the first frame (`frame1`) from BGR to grayscale.\n",
    "  - **Why?** Grayscale simplifies optical flow calculations by reducing color complexity.\n",
    "\n",
    "- **Create HSV Mask**:\n",
    "  - `np.zeros_like(frame1)`: Creates a blank array (same shape as `frame1`) initialized to zero.\n",
    "  - `hsv_mask[:,:,1] = 255`: Sets the saturation channel (`S`) to maximum for vivid coloring when converted to BGR.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Loop Through Frames**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "while True:\n",
    "    ret, frame2 = cap.read()\n",
    "    nextImg = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "```\n",
    "\n",
    "- Captures the next frame (`frame2`) and converts it to grayscale (`nextImg`).\n",
    "- **Purpose**: To compute the optical flow between the current frame (`prvsImg`) and the next frame (`nextImg`).\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Computing Optical Flow**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "flow = cv2.calcOpticalFlowFarneback(prvsImg, nextImg, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "```\n",
    "\n",
    "- **Method**: `cv2.calcOpticalFlowFarneback` calculates dense optical flow using the Farneback algorithm.\n",
    "- **Parameters**:\n",
    "  - `prvsImg`: Previous grayscale image.\n",
    "  - `nextImg`: Current grayscale image.\n",
    "  - `None`: Placeholder for optional flow initialization.\n",
    "  - `0.5`: Pyramid scale factor (reduces image size to capture large motions).\n",
    "  - `3`: Number of pyramid levels (controls motion detail).\n",
    "  - `15`: Window size for motion averaging.\n",
    "  - `3`: Number of iterations at each pyramid level.\n",
    "  - `5`: Pixel neighborhood size for polynomial expansion.\n",
    "  - `1.2`: Standard deviation for Gaussian filtering.\n",
    "  - `0`: Flags (default).\n",
    "\n",
    "- **Output**: A 2D vector field (`flow`) representing motion between the two frames.\n",
    "  - `flow[:,:,0]`: Horizontal motion.\n",
    "  - `flow[:,:,1]`: Vertical motion.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Extracting Magnitude and Angle**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "mag, ang = cv2.cartToPolar(flow[:,:,0], flow[:,:,1], angleInDegrees=True)\n",
    "```\n",
    "\n",
    "- **`cv2.cartToPolar`**: Converts Cartesian motion vectors (\\(dx, dy\\)) to polar coordinates:\n",
    "  - `mag`: Magnitude of motion (speed).\n",
    "  - `ang`: Angle of motion (direction) in degrees.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Coloring the Optical Flow**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "hsv_mask[:,:,0] = ang / 2\n",
    "hsv_mask[:,:,2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "```\n",
    "\n",
    "- **Hue (Angle)**:\n",
    "  - `hsv_mask[:,:,0]`: Represents the angle of motion (divided by 2 to fit HSV range of 0-180 degrees).\n",
    "- **Value (Magnitude)**:\n",
    "  - `cv2.normalize`: Scales the magnitude values (`mag`) to fit in the range [0, 255] for proper visualization.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Converting Back to BGR**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "bgr = cv2.cvtColor(hsv_mask, cv2.COLOR_HSV2BGR)\n",
    "cv2.imshow('frame2', bgr)\n",
    "```\n",
    "\n",
    "- **`cv2.cvtColor`**: Converts the HSV mask to BGR for display.\n",
    "- **`cv2.imshow`**: Displays the colored optical flow as a video stream.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Loop Control**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "k = cv2.waitKey(30) & 0xff\n",
    "if k == 27:\n",
    "    break\n",
    "```\n",
    "\n",
    "- **`cv2.waitKey`**: Waits for 30 ms to check for a key press.\n",
    "- **`27`**: ASCII code for the `Esc` key. If pressed, the loop terminates.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Updating for the Next Iteration**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "prvsImg = nextImg\n",
    "```\n",
    "\n",
    "- Updates `prvsImg` to the current frame (`nextImg`) for the next optical flow calculation.\n",
    "\n",
    "---\n",
    "\n",
    "## **11. Cleanup**\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "```\n",
    "\n",
    "- **`cap.release()`**: Releases the video capture resource.\n",
    "- **`cv2.destroyAllWindows()`**: Closes all OpenCV windows.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "1. Captures video from the camera.\n",
    "2. Converts frames to grayscale for processing.\n",
    "3. Computes dense optical flow using the Farneback method.\n",
    "4. Visualizes motion as color-coded flow based on direction and magnitude.\n",
    "5. Repeats for each frame until the `Esc` key is pressed.\n",
    "6. Cleans up resources and exits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
